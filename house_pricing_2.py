# -*- coding: utf-8 -*-
"""House Pricing 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K8WLnsFqNvMk8Gu5et6iMfKdcsQjwrXh
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC
from imblearn.under_sampling import RandomUnderSampler, NearMiss, TomekLinks, EditedNearestNeighbours
from imblearn.combine import SMOTETomek, SMOTEENN
import seaborn as sns
from sklearn.metrics import classification_report
# to visualise al the columns in the dataframe
pd.pandas.set_option('display.max_columns', None)

# Load the dataset
# Replace 'your_dataset.csv' with the actual dataset file path
dataset_path = "/content/train.csv"
housing_data = pd.read_csv(dataset_path)

housing_data.head()

housing_data.describe()

housing_data.info()

# Display the count of missing values for each column
missing_values_count = housing_data.isnull().sum()
print("Missing Values Count:")
print(missing_values_count[missing_values_count > 0])

# check null values in percentage - train
(housing_data.isna().sum() / len(housing_data)).sort_values(ascending=False)

# Visualize missing values using a heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(housing_data.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Values Heatmap')
plt.show()

# Define columns with missing values and their respective counts
columns_with_missing = {
    'MSZoning': 4,
    'LotFrontage': 227,
    'Alley': 1352,
    'Utilities': 2,
    'Exterior1st': 1,
    'Exterior2nd': 1,
    'MasVnrType': 16,
    'MasVnrArea': 15,
    'BsmtQual': 44,
    'BsmtCond': 45,
    'BsmtExposure': 44,
    'BsmtFinType1': 42,
    'BsmtFinSF1': 1,
    'BsmtFinType2': 42,
    'BsmtFinSF2': 1,
    'BsmtUnfSF': 1,
    'TotalBsmtSF': 1,
    'BsmtFullBath': 2,
    'BsmtHalfBath': 2,
    'KitchenQual': 1,
    'Functional': 2,
    'FireplaceQu': 730,
    'GarageType': 76,
    'GarageYrBlt': 78,
    'GarageFinish': 78,
    'GarageCars': 1,
    'GarageArea': 1,
    'GarageQual': 78,
    'GarageCond': 78,
    'PoolQC': 1456,
    'Fence': 1169,
    'MiscFeature': 1408,
    'SaleType': 1
}

# Handling missing values
for column, count in columns_with_missing.items():
    if count > 0:
        # Check if the column is numeric
        if pd.api.types.is_numeric_dtype(housing_data[column]):
            imputation_value = housing_data[column].mean()
        else:
            imputation_value = housing_data[column].mode().iloc[0]  # Use mode for categorical data

        # Fill missing values in the specified column with the chosen imputation value
        housing_data[column].fillna(imputation_value, inplace=True)

# Verify that missing values have been handled
missing_values_after_imputation = housing_data.isnull().sum()
print("Missing Values Count After Imputation:")
print(missing_values_after_imputation[missing_values_after_imputation > 0])

housing_data.head()

"""Feature Engineering

working on a regression problem where you need to predict the sale price, class imbalance may not be a concern because regression problems involve predicting continuous numerical values rather than discrete classes.
"""

# Specify the target variable (assuming it's called 'price')
target_column = 'SalePrice'
# Assume X is your feature matrix and y is your target variable
X = housing_data.drop([target_column, 'Id'], axis=1)
y = housing_data[target_column]

X.shape, y.shape

print(y.value_counts())

# Identify categorical columns
categorical_columns = X.select_dtypes(include=['object']).columns

# One-hot encode categorical columns
X_encoded = pd.get_dummies(X, columns=categorical_columns, drop_first=True)

numric_columns = housing_data.select_dtypes(include=np.number).columns.tolist()[:-1]
numric_columns

# Select only numeric columns
df_corr = housing_data.select_dtypes(include=[np.number])

# Set up the matplotlib figure
plt.subplots(figsize=(12, 9))

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(df_corr.corr(), dtype=bool))

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(df_corr.corr(), cmap='jet', mask=mask)

# Show the plot
plt.show()

"""** Handling Rare Categorical Feature**
 remove categorical variables that are present less than 1% of the observations
"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape,y_test.shape

# Define numerical and categorical features
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

for feature in categorical_features:
    labels_ordered=housing_data.groupby([feature])['SalePrice'].mean().sort_values().index
    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}
    housing_data[feature]=housing_data[feature].map(labels_ordered)

housing_data.head(10)

scaling_feature=[feature for feature in housing_data.columns if feature not in ['Id','SalePerice'] ]
len(scaling_feature)

scaling_feature

# Assuming 'train' is your DataFrame
missing_values = X_encoded.isnull().sum()
total_rows = X_encoded.shape[0]

# Create a DataFrame to display the missing values information
missing_info = pd.DataFrame({
    'Column': missing_values.index,
    'Missing Values': missing_values.values,
    'Percentage': (missing_values / total_rows) * 100
})

# Filter the DataFrame to show only columns with missing values
columns_with_missing_values = missing_info[missing_info['Missing Values'] > 0]

# Display the filtered information
print("\nColumns with Missing Values Train_set:")
print(columns_with_missing_values)

# Specify the target variable
target_column = 'SalePrice'

# Assume X is your feature matrix and y is your target variable
X = housing_data.drop([target_column, 'Id'], axis=1)
y = housing_data[target_column]

# Identify categorical columns
categorical_columns = X.select_dtypes(include=['object']).columns

# Create transformers for numerical and categorical columns
numeric_features = X.select_dtypes(include=['float64', 'int64']).columns
numeric_transformer = SimpleImputer(strategy='mean')

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))
])

# Create preprocessor for numerical and categorical columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_columns)
    ])

# Combine preprocessor with the model in a pipeline
rf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
])

# Create XGBoost Regressor
xgb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', XGBRegressor(n_estimators=100, random_state=42))
])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the Random Forest model using the pipeline
rf_pipeline.fit(X_train, y_train)

# Perform feature selection using RandomForest
sfm_rf = SelectFromModel(rf_pipeline.named_steps['regressor'], threshold=0.1)
sfm_rf.fit(rf_pipeline.named_steps['preprocessor'].transform(X_train), y_train)

# Transform the datasets with selected features for Random Forest
X_train_selected_rf = sfm_rf.transform(rf_pipeline.named_steps['preprocessor'].transform(X_train))
X_test_selected_rf = sfm_rf.transform(rf_pipeline.named_steps['preprocessor'].transform(X_test))

# Fit the XGBoost model using the pipeline
xgb_pipeline.fit(X_train, y_train)

# Perform feature selection using XGBoost
sfm_xgb = SelectFromModel(xgb_pipeline.named_steps['regressor'], threshold=0.1)
sfm_xgb.fit(xgb_pipeline.named_steps['preprocessor'].transform(X_train), y_train)

# Transform the datasets with selected features for XGBoost
X_train_selected_xgb = sfm_xgb.transform(xgb_pipeline.named_steps['preprocessor'].transform(X_train))
X_test_selected_xgb = sfm_xgb.transform(xgb_pipeline.named_steps['preprocessor'].transform(X_test))

# Print selected features for Random Forest
print("Selected Features for Random Forest:")
print(X_train.columns[sfm_rf.get_support()])

# Print selected features for XGBoost
print("Selected Features for XGBoost:")
print(X_train.columns[sfm_xgb.get_support()])

# Train and evaluate Random Forest
rf_pipeline.fit(X_train, y_train)
rf_predictions = rf_pipeline.predict(X_test)
rf_mse = mean_squared_error(y_test, rf_predictions)
print(f"Random Forest MSE: {rf_mse}")

from sklearn.metrics import confusion_matrix
threshold = 0.5
classification_predictions = (rf_predictions > threshold).astype(int)
# Create confusion matrix
cm = confusion_matrix(y_test, classification_predictions)
print("Confusion Matrix:")
print(cm)

